{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a7ff10",
   "metadata": {},
   "source": [
    "# TDA (Topological Data Analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3de71d",
   "metadata": {},
   "source": [
    "# Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948f49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "#import Production_Nikkei.tools as tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849713d",
   "metadata": {},
   "source": [
    "# Get S&P componants\n",
    "\n",
    "Pour cet exemple de **TDA** nous allons crééer un cluster d'actions liées entre elles grace à l'analyse de leurs returns\n",
    "\n",
    "Dans un premier temps, extrayons les composants du S&P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59dd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_index_constituant(path = \"./sp_components.txt\",from_base=1):\n",
    "    fd = open(path, 'r')\n",
    "    index = list(filter(lambda x : x,fd.read().split('\\n')))\n",
    "    fd.close()\n",
    "\n",
    "    return index\n",
    "\n",
    "spy_list = list_index_constituant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf236a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_list = sorted(spy_list)\n",
    "s = spy_list[:10]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5fa95",
   "metadata": {},
   "source": [
    "Récuperrons les daily returns de ces tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3409e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(\"./sp_value.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fcd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400453f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e475f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data = res[res.field == 'daily_return']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa6483",
   "metadata": {},
   "source": [
    "On fait pivoter le dataFrame afn d'avoir les tickers en header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f103bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data = sp_data[[\"ticker\", 'value']].pivot( columns='ticker', values = 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37943998",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3095a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['AAPL US Equity', 'MSFT US Equity', 'GOOG US Equity']\n",
    "\n",
    "data_features = sp_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d48367a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data_features.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba8618",
   "metadata": {},
   "source": [
    "# LOG RETURNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b75bae3",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow; font-weight:bold\">PCA + UMAP</span>\n",
    "\n",
    "## <span style=\"color:orange; font-weight:bold\">PCA</span>\n",
    "\n",
    "### Pourquoi utiliser PCA ?\n",
    "- Pour éliminer le bruit.\n",
    "- Pour simplifier les données.\n",
    "- Pour garder uniquement l’information avec le plus de variance.\n",
    "\n",
    "### Que fait PCA(n_components=0.8) ?\n",
    "- Il conserve les composantes principales qui expliquent 80% de la variance totale.\n",
    "- Au lieu de 1000 dimensions (1 par jour), on se retrouve peut-être avec 10 ou 20 composantes principales, linéaires, non redondantes.\n",
    "\n",
    "### Exemple\n",
    "Imaginons que nous avionss 1000 log returns → PCA garde peut-être 15 directions principales :\n",
    "- log_returns: (200 stocks x 1000 jours)\n",
    "- reduced:     (200 stocks x ~15 composantes principales)\n",
    "\n",
    "## <span style=\"color:orange; font-weight:bold\">UMAP</span>\n",
    "\n",
    "### Pourquoi UMAP après PCA ?\n",
    "PCA est linéaire. Il ne détecte pas bien les formes non linéaires.\n",
    "\n",
    "- UMAP capture la structure topologique : forme des nuages de points, distances locales et globales, etc.\n",
    "- UMAP projette les données en 2D ou 3D, tout en respectant la forme intrinsèque de l’espace original.\n",
    "\n",
    "### Que fait UMAP(n_components=2) ?\n",
    "- Il projette les données PCA réduites dans un espace 2D.\n",
    "- Chaque stock devient un point dans un plan, qui reflète sa similarité topologique avec les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d44a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Il conserve les composantes principales qui expliquent 80% de la variance totale.\n",
    "\n",
    "# Au lieu de 1000 dimensions (1 par jour), on se retrouve peut-être avec 10 ou 20 composantes principales, linéaires, non redondantes.\n",
    "pca = PCA(n_components=0.8, svd_solver='full', random_state=42)\n",
    "\n",
    "\n",
    "reduced = pca.fit_transform(data_features.T)\n",
    "\n",
    "# Avec UMAP on passe d'une 10aine de dimensions a juste 2 Dimensions\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "embedding = umap_model.fit_transform(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57df426",
   "metadata": {},
   "source": [
    "`embedding` contiennt le tableau qui contient les stocks transofrmés en points. Pour rappel, les dimensions ont été réduites à deux (X, Y) et chaque stock peut maintenant être assimilié à un point.\n",
    "\n",
    "Sa proximité avec d'autres stocks peut traduire une relation interessante (corrlation implicite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3342f1",
   "metadata": {},
   "source": [
    "Des formes et des **amas naturels** (clusters) apparaissent\n",
    "\n",
    "A partir de ces coordonnées, on contruira des **clusters** avec `DBSCAN` , puis un **graphe topologique** avec `Mapper`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc7fea",
   "metadata": {},
   "source": [
    "# Clusters with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1832d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmapper as km\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# le mapper est l’objet principal qui va :\n",
    "#   projeter, découper et organiser les données,\n",
    "#   construire un graphe qui approxime la \"forme\" de la donnée.\n",
    "\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "\n",
    "projected_data = embedding\n",
    "graph = mapper.map(\n",
    "    projected_data,\n",
    "    data_features.T,\n",
    "    cover=km.Cover(n_cubes=10, perc_overlap=0.5),\n",
    "    clusterer=DBSCAN(metric='correlation', eps=0.3, min_samples=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfe0bb",
   "metadata": {},
   "source": [
    "la fonction `map` de notre **mapper** prends en arguments:\n",
    "- `projected_data` qui sont les coordonnées en 2D de chaque ticker\n",
    "- `log_returns.T` les returns avec les tickers en index\n",
    "- `clusterer=DBSCAN`et `cover=km.Cover()` seront expliqués en bas\n",
    "\n",
    "<span style=\"color:orange; font-weight:bold\">Covering: </span>\n",
    "L'argument `cover=km.Cover(n_cubes=10, perc_overlap=0.5)`divise l’espace 2D en grilles carrées (cubes) : ici 10x10\n",
    "\n",
    "Chaque \"cube\" est une sous-région de l’espace projeté.\n",
    "\n",
    "`n_cubes=10`: l'espace est divisé en 10x10 cubes\n",
    "`perc_overlap=0.5` : les cubes se chevauchent à 50%. Cela permet d’avoir des clusters partagés, et de capter des structures connectées.\n",
    "\n",
    "Cela permet à Mapper de connecter des clusters partiels pour reconstruire la forme globale (ex : boucle, branche, cluster étendu…)\n",
    "\n",
    "<span style=\"color:orange; font-weight:bold\">Clustering avec DBSCAN</span>\n",
    "\n",
    "Pour chaque cube (sous-région), on applique DBSCAN :\n",
    "\n",
    "- Un algorithme robuste aux formes irrégulières\n",
    "- Pas besoin de spécifier le nombre de clusters à l’avance\n",
    "- Utilise ici une distance de corrélation, adaptée à des séries temporelles (retours).\n",
    "\n",
    "Paramètres importants :\n",
    "- metric='correlation' : distance adaptée à des données de type rendement.\n",
    "- eps=0.3 : seuil de proximité. Plus petit → plus de petits clusters.\n",
    "- min_samples=3 : au moins 3 points pour former un cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb2702",
   "metadata": {},
   "source": [
    "Sauvegarde visuelle du graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b276c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.visualize(graph, path_html=\"portfolio.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cbcf9",
   "metadata": {},
   "source": [
    "Chaque nœud est un cluster local d’actions.\n",
    "\n",
    "Si deux nœuds ont des actions en commun (à cause du recouvrement), on trace un lien (edge).\n",
    "\n",
    "Résultat : un graphe de clusters connectés, qui représente la structure topologique des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1278424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = graph['nodes']  # chaque noeud contient les index des stocks\n",
    "clusters = [data_features.T.index[list(indices)] for indices in nodes.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c914b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_target = 45\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    if i == cluster_target:\n",
    "        sp_data[list(clusters[i])].cumsum().plot()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_target = 45\n",
    "\n",
    "sp_data[list(clusters[cluster_target])].cumsum().plot(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19411628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp_data[list(clusters[cluster_target])].sum(axis=1).cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44b298",
   "metadata": {},
   "source": [
    "# Portolio  custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deec1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infos = pd.read_csv(\"./sp500_gics_info_2010_2025.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_arr = []\n",
    "from collections import defaultdict\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    tmp = clusters[i]\n",
    "    dic = defaultdict(int)\n",
    "    for j in tmp:\n",
    "        try:\n",
    "            sector = df_infos.loc[j.split(\" \")[0]][1]\n",
    "        except:\n",
    "            sector = \"other\"\n",
    "        finally:\n",
    "            dic[sector] += 1\n",
    "    for j in dic:\n",
    "        dic[j] /= len(tmp)\n",
    "    ratio_arr.append((dic, i))\n",
    "\n",
    "ratio_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "182bfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_from_target(ratio_arr, target):\n",
    "\n",
    "    def calculate_cluster_score(x):\n",
    "        dic = x[0]\n",
    "        ans = 0\n",
    "        for i in target:\n",
    "            ans += dic[i] * target[i]\n",
    "        return ans\n",
    "\n",
    "    ans = sorted(ratio_arr, key=calculate_cluster_score, reverse=1)\n",
    "    return ans\n",
    "target = {\"Health Care\": 1}\n",
    "\n",
    "sah = get_clusters_from_target(ratio_arr, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_clustors_from_id_sorted(arr, index):\n",
    "\n",
    "    sp_data[list(clusters[arr[index][1]])].cumsum().plot()\n",
    "\n",
    "\n",
    "sash = print_raw_clustors_from_id_sorted(sah, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_agg_clustors_from_id_sorted(arr, index):\n",
    "\n",
    "    sp_data[list(clusters[arr[index][1]])].sum(axis=1).cumsum().plot()\n",
    "\n",
    "\n",
    "sash = print_agg_clustors_from_id_sorted(sah, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b97213f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data[list(clusters[cluster_target])].cumsum().plot(backend='plotly').update_layout(width=1400).write_image(f\"cluster_{cluster_target}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf454f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data[list(clusters[cluster_target])].cumsum().plot(backend='plotly').update_layout(width=1400).write_html(f\"cluster_{cluster_target}.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da310c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data[['AFL US Equity', 'AIG US Equity', 'AYI US Equity', 'ECL US Equity', 'HLT US Equity', 'HST US Equity', 'MNST US Equity', 'ODFL US Equity', 'PSA US Equity', 'R US Equity', 'WAT US Equity', 'YUM US Equity']].cumsum().plot(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_with_target(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847a28d",
   "metadata": {},
   "source": [
    "# Resumé\n",
    "\n",
    "L\"zcrendements d’actions sont des points dans l’espace.\n",
    "On les projette en 2D (UMAP). On pose un filet en carré sur cet espace (Cover).\n",
    "Dans chaque carré, on trouve des amas denses (clusters) avec DBSCAN.\n",
    "Puis on relie les amas qui se chevauchent, obtenant ainsi une carte de la topologie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776c426",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "On essaiera de créer un portefeuille dont les poids refleteront le lien de chaque ticker avec les autres élements de cet univers.\n",
    "\n",
    "Dans ce portfefeuille, on aura:\n",
    "- Le capital est réparti équitablement entre les grands clusters (composantes connexes du graphe).\n",
    "- Puis réparti entre les sous-clusters (nœuds) à l’intérieur de chaque grand cluster.\n",
    "- Puis réparti entre les actions dans chaque sous-cluster.\n",
    "\n",
    "C’est une stratégie de pondération hiérarchique.\n",
    "\n",
    "On aura comme poids pour nos stocks\n",
    "\n",
    "$$poids_{action}\\ = \\frac{1}{\\text {nb clusters}} * \\frac{1}{\\text {nb sous-clusters dans cluster}} * \\frac{1}{\\text {nb actions dans sous clusters}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab0504",
   "metadata": {},
   "source": [
    "## Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_symbols = [\n",
    "\n",
    "    [  # Grand cluster 1 (50%)\n",
    "        ['AAPL', 'MSFT'],     # Sous-cluster 1 (25%)\n",
    "        ['GOOG'],             # Sous-cluster 2 (25%)\n",
    "    ],\n",
    "    \n",
    "    [  # Grand cluster 2 (50%)\n",
    "        ['TSLA', 'NVDA'],     # Sous-cluster 3 (25%)\n",
    "        ['AMZN', 'META'],     # Sous-cluster 4 (25%)\n",
    "    ],\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71478668",
   "metadata": {},
   "source": [
    "Weighting will be \n",
    "\n",
    "AAPL     0.125\n",
    "\n",
    "MSFT     0.125\n",
    "\n",
    "GOOG     0.25\n",
    "\n",
    "TSLA     0.125\n",
    "\n",
    "NVDA     0.125\n",
    "\n",
    "AMZN     0.125\n",
    "\n",
    "META     0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b245251",
   "metadata": {},
   "source": [
    "Cette fonction calcul le weighting recursivement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_distribution(clustered_symbols):\n",
    "    weights = {}\n",
    "    def assign_weights(nested_list, level=1):\n",
    "        n = len(nested_list)\n",
    "        w = 1 / n\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, pd.Index):\n",
    "                assign_weights(item, level + 1)\n",
    "            else:\n",
    "#                print(item)\n",
    "                weights[item] = weights.get(item, 0) + w / (2 ** (level - 1))\n",
    "    assign_weights(clustered_symbols)\n",
    "    return pd.Series(weights) / sum(weights.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160613ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weight_distribution(clusters)\n",
    "print(weights.sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1470d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clusters:\n",
    "    if len(i) == 17:\n",
    "        print(type(i))\n",
    "        print(isinstance(i, pd.Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897aa799",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns[weights.index] * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tda_portfolio = log_returns[weights.index]\n",
    "tda_portfolio = tda_portfolio * weights\n",
    "tda_portfolio['sum_log_return'] = tda_portfolio.sum(axis=1)\n",
    "tda_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tda_portfolio['sum_log_return'].cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98544a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns.sum(axis=1).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sharp(strat):\n",
    "    return strat.mean()/(strat.std() * np.sqrt(252) )\n",
    "print(compute_sharp(log_returns.sum(axis=1)))\n",
    "print(compute_sharp(tda_portfolio['sum_log_return']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
